from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.tree import DecisionTreeClassifier
import warnings                             
warnings.filterwarnings("ignore") 

seed=42

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
   
def DropNA(train_X, test):
    train_X.dropna(axis=1, how='all', inplace=True)
    test = test[train_X.columns]
    return train_X, test
    
def DropDuplicateColumns(train_X, test): #중복컬럼 제거
    train_X = train_X.T.drop_duplicates(keep='first').T
    test = test[train_X.columns]
    return train_X, test

def RemoveOneValueColumn(train_X, test): #하나의 값만 가진 열 제거
    for col in [x for x in train_X.columns if 'X_' in x]:
        if len(train_X[col].value_counts())==1:
            train_X.drop(col, axis=1, inplace=True)
        test = test[train_X.columns]
    return train_X,test
    
def CombineLineProduct(train_X, test): #제품에서 라인이 같은것 묶기!
    target_col = ['PRODUCT_CODE','LINE']
    new_col = 'PROD_CODE_AND_LINE'
    train_X[new_col] = train_X['PRODUCT_CODE']+train_X['LINE']
    train_X.drop(target_col,axis=1, inplace=True)
    test[new_col] = test['PRODUCT_CODE']+test['LINE']
    test.drop(target_col,axis=1, inplace=True)
    return train_X, test, new_col
    
def OnehotEncod(train_X, test, new_col):#원핫인코딩
    ohe = OneHotEncoder(sparse=False)
    ohe.fit(train_X[new_col].values.reshape(-1,1))
    one_hot_encoded_train = ohe.transform(train_X[new_col].values.reshape(-1,1))
    for label in np.unique(test[new_col]):
        if label not in ohe.categories_[0]: 
                ohe.classes_[0] = ohe.append(ohe.categories_[0], label)
    one_hot_encoded_test = ohe.transform(test[new_col].values.reshape(-1,1))
    ohe_train = pd.DataFrame(one_hot_encoded_train, columns = ohe.categories_[0])
    ohe_test = pd.DataFrame(one_hot_encoded_test, columns = ohe.categories_[0])
    ohe_train = ohe_train.astype(int)
    ohe_test = ohe_test.astype(int)
    train_X.drop(columns=[new_col], axis=1, inplace=True)
    test.drop(columns=[new_col], axis=1, inplace=True)
    train_X = pd.concat([train_X, ohe_train], axis=1)
    test = pd.concat([test, ohe_test], axis=1)
    
    return train_X, test   

def fillNaN(train_X, test): #결측치 처리
    train_X = train_X.fillna(0)
    test = test.fillna(0)
    return train_X, test

def scalingRobust(train_X, test): #스케일링
    scaler = RobustScaler()
    target_col = list(filter(lambda x: x.startswith('X_'), train_X.columns))
    scaler.fit(train_X[target_col])
    train_X[target_col] = scaler.transform(train_X[target_col])
    test[target_col] = scaler.transform(test[target_col])
    return train_X, test

def preprocess(train_X, train_y, test):
    train_X, test = DropNA(train_X, test)
    train_X, test = DropDuplicateColumns(train_X, test)
    train_X, test = RemoveOneValueColumn(train_X, test)
    train_X, test, new_col = CombineLineProduct(train_X, test)
    train_X, test = OnehotEncod(train_X, test, new_col)
    train_X, test = fillNaN(train_X, test)
    train_X, test = scalingRobust(train_X, test)
    return train_X.values, train_y.values, test.values

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = seed)
xgb = XGBClassifier
xgb_params = {'max_dapth' : [3,4,5,6,7,8,9],
             'gamma':[0,0.001,0.01,0.1,1],
             'learning_rate' : [0.01,0.05,0.1],
             }
gs_xgb = GridSearchCV(xgb,xgb_params,refit=True)

xgb_param_grid={
    'n_estimators' : [100,200,300,400,500],
    'learning_rate' : [0.01,0.05,0.1,0.15],
    'max_depth' : [3,5,7,10,15],
    'gamma' : [0,1,2,3],
    'colsample_bytree' : [0.8,0.9],
    
}
xgb_grid=GridSearchCV(xgb, param_grid = xgb_param_grid, scoring="f1_macro", n_jobs=-1, verbose = 2)

def main():
    seed_everything(seed)
    train_df = pd.read_csv('/kaggle/input/dacondata/train.csv')
    test_df = pd.read_csv('/kaggle/input/dacondata/test.csv')

    train_X = train_df.drop(columns=['PRODUCT_ID','Y_Class','Y_Quality'])
    train_y = train_df['Y_Class']
    test = test_df.drop(columns=['PRODUCT_ID'])

    train_X, train_y, test = preprocess(train_X, train_y, test)
    
#     lgbm = LGBMClassifier(
#     task = 'predict',
#     application = 'multiclass',
#     boosting_type="gbdt",
#     num_iterations = 2500,
#     learning_rate = 0.09,
#     num_leaves=20,
#     tree_learner='feature',
#     max_depth =10,
#     min_data_in_leaf=76,
#     bagging_fraction = 0.8577,
#     bagging_freq = 11,
#     reg_sqrt='True',
#     feature_fraction = 0.58,
#     random_state=27
#     )

    xgb = XGBClassifier(seed = 27,
                    learning_rate =0.1,
                    n_estimators=1000,
                    max_depth=9,
                    min_child_weight=3,
                    gamma=0,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    colsample_bylevel=0.9,
                    objective= 'multi:softmax',
                    nthread=4,
                    booster = 'gbtree',
                    scale_pos_weight = 1
                    )
    
    

#     cat = CatBoostClassifier(one_hot_max_size=2, random_seed=42,
#                                 task_type='GPU', iterations=4500, learning_rate=0.05)
    

#     eclf = VotingClassifier(estimators=[('xg', xgb), ('lg', lgbm)],voting='hard',weights=[8,2])
#     eclf = eclf.fit(train_X,train_y)
    xgb.fit(train_X, train_y)

    preds = xgb.predict(test)
    submit = pd.read_csv('/kaggle/input/dacondata/sample_submission.csv')
    submit['Y_Class'] = preds
    submit.to_csv('submit.csv', index=False)
# def training(train_X, train_y, test):
#     models = [
#         LGBMClassifier(objective='multiclass', random_state=seed),
#         CatBoostClassifier(objective='MultiClass',
#                             task_type='GPU',
#                             one_hot_max_size=2, random_seed=42,
#                             iterations=4000, early_stopping_rounds=50,
#                             learning_rate=0.05, verbose=False
#                             ),
#         XGBClassifier(random_state=seed),
#     ]
    
#     [x.fit(train_X, train_y) for x in models]
    
#     return models

# def predict(models, test, mode, weights):
#     if mode == "hard":
#         preds = np.asarray([x.predict(test).reshape(-1) for x in models]).T
#         res = np.apply_along_axis(
#             lambda x: np.argmax(np.bincount(x, weights=weights)),
#             axis=1,
#             arr=preds
#         )
#     if mode == "soft":
#         preds = np.asarray([x.predict_proba(test) for x in models])
#         res = np.zeros(preds[0].shape)
#         for pred, weight in zip(preds, weights):
#             res = res + pred*weight
#         res = np.argmax(preds, axis=0)
#     return res

# def submission(preds):
#     submit = pd.read_csv('/kaggle/input/dacondata/train.csv')
#     submit['Y_Class'] = preds
#     submit.to_csv('submit.csv', index=False)

# def main():
#     seed_everything(seed)
      
#     train_df = pd.read_csv('/kaggle/input/dacondata/train.csv')
    
#     test_df = pd.read_csv('/kaggle/input/dacondata/test.csv')

      
#     train_X = train_df.drop(columns=['PRODUCT_ID','Y_Class','Y_Quality'])
    
#     train_y = train_df['Y_Class']
#       test = test_df.drop(columns=['PRODUCT_ID'])

    
#     train_X, train_y, test = preprocess(train_X, train_y, test)
    

#     train_X, train_y = load_data.split_data_label(train)
#     test, _ = load_data.split_data_label(test, False)

#     train_X, train_y, test = preprocess(train_X, train_y, test)
    
#     models = training(train_X, train_y, test)
#     preds = predict(models, test, "hard", [2,2,1])
#     submission(preds, datapath, 'hard')

if __name__ == "__main__": 
    main()
    
